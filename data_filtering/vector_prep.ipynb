{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "717ea29e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded 18 sheets from 'DUOLIFE Ingredients.xlsx'.\n",
      "\n",
      "Available data frames (with sanitized names):\n",
      "- Sheet1\n",
      "- Sheet2\n",
      "- Sheet3\n",
      "- Sheet4\n",
      "- Import Duolife products\n",
      "- Import Duolife Ingredients\n",
      "- Products\n",
      "- Ingredients\n",
      "- Cosmetics\n",
      "- Ingredients v2\n",
      "- Old Business Q&A\n",
      "- Club Ranks\n",
      "- Career Structure\n",
      "- Compensatory Bonus\n",
      "- Rentier Bonus\n",
      "- MB Bonus\n",
      "- Sheet17\n",
      "- Incentive Program\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import uuid\n",
    "import re\n",
    "\n",
    "# --- Configuration ---\n",
    "# Ensure this path points correctly to your Excel file\n",
    "file_path = 'DUOLIFE Ingredients.xlsx'\n",
    "\n",
    "# --- Load all sheets from the Excel file ---\n",
    "# This creates a dictionary where keys are sheet names and values are DataFrames\n",
    "try:\n",
    "    xls = pd.ExcelFile(file_path)\n",
    "    sheet_names = xls.sheet_names\n",
    "    # Sanitize sheet names to make them easier to use as dictionary keys\n",
    "    sanitized_sheet_names = {sheet: sheet.replace('.csv', '').strip() for sheet in sheet_names}\n",
    "    data_frames = {sanitized_sheet_names[sheet]: pd.read_excel(xls, sheet_name=sheet) for sheet in sheet_names}\n",
    "    \n",
    "    print(f\"Successfully loaded {len(sheet_names)} sheets from '{file_path}'.\")\n",
    "    print(\"\\nAvailable data frames (with sanitized names):\")\n",
    "    for name in data_frames.keys():\n",
    "        print(f\"- {name}\")\n",
    "        \n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The file '{file_path}' was not found. Please check the path and try again.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while loading the Excel file: {e}\")\n",
    "\n",
    "# This master list will hold all our final structured data chunks\n",
    "all_chunks = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4adb74d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"Cleans input text by removing extra whitespace, newlines, and leading/trailing spaces.\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    # Replace multiple whitespace characters (including newlines) with a single space\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.strip()\n",
    "\n",
    "def create_chunk(text_to_embed, metadata):\n",
    "    \"\"\"Creates a standardized dictionary for a data chunk if the text is valid.\"\"\"\n",
    "    cleaned_text = clean_text(text_to_embed)\n",
    "    # We only create a chunk if there is meaningful text to embed\n",
    "        \n",
    "    # Generate a unique ID for each chunk\n",
    "    chunk_id = str(uuid.uuid4())\n",
    "    \n",
    "    return {\n",
    "        \"chunk_id\": chunk_id,\n",
    "        \"text_to_embed\": cleaned_text,\n",
    "        \"metadata\": metadata\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b62e3378",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Processing Domain: Products & Ingredients ---\n",
      "Processed 57 products into multiple chunks.\n",
      "Processed 281 ingredients.\n"
     ]
    }
   ],
   "source": [
    "def process_products_and_ingredients(data_frames, all_chunks):\n",
    "    print(\"\\n--- Processing Domain: Products & Ingredients ---\")\n",
    "    \n",
    "    # --- Process Products ---\n",
    "    df_products = data_frames.get('Products')\n",
    "    if df_products is not None:\n",
    "        product_count = 0\n",
    "        # Define the columns that contain text we want to chunk separately\n",
    "        product_columns_to_chunk = [\n",
    "            'Description', 'Intended Use', 'Health Effects', \n",
    "            'Formulation Advantages', 'How to Use', 'Contraindications', 'Interactions'\n",
    "        ]\n",
    "\n",
    "        for index, row in df_products.iterrows():\n",
    "            product_name = clean_text(row.get('Product Name'))\n",
    "            if not product_name:\n",
    "                continue\n",
    "            \n",
    "            product_count += 1\n",
    "            base_metadata = {\n",
    "                \"domain\": \"Products\",\n",
    "                \"source_file\": \"Products.csv\",\n",
    "                \"primary_entity_id\": str(row.get('Product ID', '')),\n",
    "                \"primary_entity_name\": product_name,\n",
    "                \"type\": clean_text(row.get('Type', '')),\n",
    "                \"link\": clean_text(row.get('Link', ''))\n",
    "            }\n",
    "            \n",
    "            for col in product_columns_to_chunk:\n",
    "                text = row.get(col)\n",
    "                if pd.notna(text):\n",
    "                    metadata = base_metadata.copy()\n",
    "                    metadata['section'] = col.replace(' ', '_') # Use snake_case\n",
    "                    chunk = create_chunk(str(text), metadata)\n",
    "                    if chunk:\n",
    "                        all_chunks.append(chunk)\n",
    "        print(f\"Processed {product_count} products into multiple chunks.\")\n",
    "\n",
    "    # --- Process Ingredients ---\n",
    "    df_ingredients = data_frames.get('Ingredients')\n",
    "    if df_ingredients is not None:\n",
    "        for index, row in df_ingredients.iterrows():\n",
    "            text = row.get('Full Description')\n",
    "            if pd.notna(text):\n",
    "                metadata = {\n",
    "                    \"domain\": \"Ingredients\",\n",
    "                    \"source_file\": \"Ingredients.csv\",\n",
    "                    \"primary_entity_id\": str(row.get('Ingredient ID', '')),\n",
    "                    \"primary_entity_name\": clean_text(row.get('Ingredient Name', '')),\n",
    "                    \"section\": \"Description\"\n",
    "                }\n",
    "                chunk = create_chunk(str(text), metadata)\n",
    "                if chunk:\n",
    "                    all_chunks.append(chunk)\n",
    "        print(f\"Processed {len(df_ingredients)} ingredients.\")\n",
    "\n",
    "# Execute the function\n",
    "process_products_and_ingredients(data_frames, all_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e83e164c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Processing Domain: Cosmetics ---\n",
      "Processed 20 cosmetic products.\n",
      "Processed 35 cosmetic ingredients.\n"
     ]
    }
   ],
   "source": [
    "def process_cosmetics(data_frames, all_chunks):\n",
    "    print(\"\\n--- Processing Domain: Cosmetics ---\")\n",
    "    \n",
    "    # --- Process Cosmetics Products ---\n",
    "    df_cosmetics = data_frames.get('Cosmetics')\n",
    "    if df_cosmetics is not None:\n",
    "        df_cosmetics.columns = [col.strip() for col in df_cosmetics.columns]\n",
    "        \n",
    "        for index, row in df_cosmetics.iterrows():\n",
    "            text = row.get('Description')\n",
    "            product_name = clean_text(row.get('Product Name'))\n",
    "            if not product_name or pd.isna(text):\n",
    "                continue\n",
    "\n",
    "            # Dynamically find all ingredient-related columns and gather the values\n",
    "            ingredients_list = []\n",
    "            for col_name in df_cosmetics.columns:\n",
    "                if 'Ingredient IDs' in col_name and pd.notna(row[col_name]):\n",
    "                    ingredients_list.append(clean_text(row[col_name]))\n",
    "\n",
    "            metadata = {\n",
    "                \"domain\": \"Cosmetics\",\n",
    "                \"source_file\": \"Cosmetics .csv\",\n",
    "                \"primary_entity_id\": str(row.get('Product ID', '')),\n",
    "                \"primary_entity_name\": product_name,\n",
    "                \"section\": \"Description\",\n",
    "                \"type\": clean_text(row.get('Type', '')),\n",
    "                \"ingredients_list\": ingredients_list\n",
    "            }\n",
    "            chunk = create_chunk(str(text), metadata)\n",
    "            if chunk:\n",
    "                all_chunks.append(chunk)\n",
    "        print(f\"Processed {len(df_cosmetics)} cosmetic products.\")\n",
    "\n",
    "    # --- Process Cosmetic Ingredients (v2) ---\n",
    "    df_ingredients_v2 = data_frames.get('Ingredients v2')\n",
    "    if df_ingredients_v2 is not None:\n",
    "        for index, row in df_ingredients_v2.iterrows():\n",
    "            text = row.get('Description')\n",
    "            if pd.notna(text):\n",
    "                metadata = {\n",
    "                    \"domain\": \"Ingredients\",\n",
    "                    \"source_file\": \"Ingredients v2.csv\",\n",
    "                    \"primary_entity_id\": str(row.get('Ingredient ID', '')),\n",
    "                    \"primary_entity_name\": clean_text(row.get('Ingredient Name', '')),\n",
    "                    \"section\": \"Description\"\n",
    "                }\n",
    "                chunk = create_chunk(str(text), metadata)\n",
    "                if chunk:\n",
    "                    all_chunks.append(chunk)\n",
    "        print(f\"Processed {len(df_ingredients_v2)} cosmetic ingredients.\")\n",
    "\n",
    "# Execute the function\n",
    "process_cosmetics(data_frames, all_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4fe004c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Processing Domain: Business Model ---\n",
      "Processed 39 Q&A entries.\n",
      "Processed 4 incentive programs.\n"
     ]
    }
   ],
   "source": [
    "def process_business_model(data_frames, all_chunks):\n",
    "    print(\"\\n--- Processing Domain: Business Model ---\")\n",
    "\n",
    "    # --- Process Business Q&A ---\n",
    "    df_qa = data_frames.get('Old Business Q&A')\n",
    "    if df_qa is not None:\n",
    "        for index, row in df_qa.iterrows():\n",
    "            question = clean_text(row.get('Question', ''))\n",
    "            answer = clean_text(row.get('Answer', ''))\n",
    "            \n",
    "            if not question or not answer:\n",
    "                continue\n",
    "\n",
    "            text_to_embed = f\"Question: {question} Answer: {answer}\"\n",
    "            keywords = [kw.strip() for kw in str(row.get('Keywords', '')).split(',') if kw.strip()]\n",
    "\n",
    "            metadata = {\n",
    "                \"domain\": \"Business_Model\",\n",
    "                \"source_file\": \"Old Business Q&A.csv\",\n",
    "                \"primary_entity_name\": question,\n",
    "                \"section\": \"Q&A\",\n",
    "                \"tags\": keywords\n",
    "            }\n",
    "            chunk = create_chunk(text_to_embed, metadata)\n",
    "            if chunk:\n",
    "                all_chunks.append(chunk)\n",
    "        print(f\"Processed {len(df_qa)} Q&A entries.\")\n",
    "\n",
    "    # --- Process Incentive Programs ---\n",
    "    df_incentives = data_frames.get('Incentive Program')\n",
    "    if df_incentives is not None:\n",
    "        # Note the typo \"Discription\" in the original column name\n",
    "        text_col_name = next((col for col in df_incentives.columns if 'Discription' in col), None)\n",
    "        if text_col_name:\n",
    "            for index, row in df_incentives.iterrows():\n",
    "                text = row.get(text_col_name)\n",
    "                program_name = clean_text(row.get('Program', ''))\n",
    "                if pd.notna(text) and program_name:\n",
    "                    metadata = {\n",
    "                        \"domain\": \"Business_Model\",\n",
    "                        \"source_file\": \"Incentive Program.csv\",\n",
    "                        \"primary_entity_name\": program_name,\n",
    "                        \"section\": \"Program_Details\"\n",
    "                    }\n",
    "                    chunk = create_chunk(str(text), metadata)\n",
    "                    if chunk:\n",
    "                        all_chunks.append(chunk)\n",
    "            print(f\"Processed {len(df_incentives)} incentive programs.\")\n",
    "\n",
    "# Execute the function\n",
    "process_business_model(data_frames, all_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f05c3ec9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Processing Domain: Ranks & Compensation ---\n",
      "Processed 20 club ranks.\n",
      "Processed 14 career structure levels.\n"
     ]
    }
   ],
   "source": [
    "def process_ranks_and_compensation(data_frames, all_chunks):\n",
    "    print(\"\\n--- Processing Domain: Ranks & Compensation ---\")\n",
    "\n",
    "    # --- Process Club Ranks ---\n",
    "    df_ranks = data_frames.get('Club Ranks')\n",
    "    if df_ranks is not None:\n",
    "        df_ranks.dropna(how='all', inplace=True)\n",
    "        for index, row in df_ranks.iterrows():\n",
    "            eng_position = clean_text(row.get('ENG Position', ''))\n",
    "            if not eng_position: continue\n",
    "\n",
    "            # Create a natural language sentence for semantic search\n",
    "            text_to_embed = (\n",
    "                f\"The rank of {eng_position} (shortcut: {row.get('Shortcut', 'N/A')}) requires a minimum monthly activity of {row.get('Minimum Monthly Activity (P)', 'N/A')}P. \"\n",
    "                f\"It requires {row.get('Minimum Number of Active Lines', 'N/A')} active lines and a total activity of {row.get('Total Activity', 'N/A')}P. \"\n",
    "                f\"Average earnings range from {row.get('Base Earnings (in P)', 'N/A')} to {row.get('Max Earnings (in P)', 'N/A')} PP. \"\n",
    "                f\"Associated Bonus: {row.get('Bonus', 'None')}.\"\n",
    "            )\n",
    "            \n",
    "            # Store the structured data in metadata for precise filtering\n",
    "            metadata = {key.replace(' ', '_').lower(): val for key, val in row.items()}\n",
    "            metadata['domain'] = \"Ranks\"\n",
    "            metadata['source_file'] = \"Club Ranks.csv\"\n",
    "            metadata['primary_entity_name'] = eng_position\n",
    "            metadata['section'] = \"Rank_Requirements\"\n",
    "            \n",
    "            chunk = create_chunk(text_to_embed, metadata)\n",
    "            if chunk:\n",
    "                all_chunks.append(chunk)\n",
    "        print(f\"Processed {len(df_ranks)} club ranks.\")\n",
    "\n",
    "    # --- Process Career Structure ---\n",
    "    df_career = data_frames.get('Career Structure')\n",
    "    if df_career is not None:\n",
    "        df_career.dropna(how='all', inplace=True)\n",
    "        for index, row in df_career.iterrows():\n",
    "            position = clean_text(row.get('Position', ''))\n",
    "            if not position: continue\n",
    "\n",
    "            text_to_embed = (\n",
    "                f\"For the {position} career position, the minimum points turnover is {row.get('Minimum Points Turnover', 'N/A')}. \"\n",
    "                f\"The structure commission is {row.get('Structure Commission', 0)*100:.2f}%. \"\n",
    "                f\"This level includes a share of the global points turnover and may include a Compensatory Bonus and Rentier Bonus.\"\n",
    "            )\n",
    "            \n",
    "            metadata = {key.replace(' ', '_').lower().replace('%', 'percent'): val for key, val in row.items()}\n",
    "            metadata['domain'] = \"Compensation\"\n",
    "            metadata['source_file'] = \"Career Structure.csv\"\n",
    "            metadata['primary_entity_name'] = position\n",
    "            metadata['section'] = \"Career_Details\"\n",
    "            \n",
    "            chunk = create_chunk(text_to_embed, metadata)\n",
    "            if chunk:\n",
    "                all_chunks.append(chunk)\n",
    "        print(f\"Processed {len(df_career)} career structure levels.\")\n",
    "\n",
    "# Execute the function\n",
    "process_ranks_and_compensation(data_frames, all_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5b807d44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- PROCESSING COMPLETE ---\n",
      "Total number of structured data chunks created: 757\n",
      "\n",
      "--- EXAMPLE CHUNKS ---\n",
      "\n",
      "--- Example from Domain: Products ---\n",
      "{\n",
      "  \"chunk_id\": \"76db44c9-3fa9-4bd1-af61-15fb9a511f48\",\n",
      "  \"text_to_embed\": \"100% natural dietary supplements, created for people wishing to stay in a good physical and mental shape. Additional energy for the whole day of intense work and valuable support for calming down and regeneration of the body at night. A combination of as many as 26 extracts and fruit juices, to support functions of the cardiovascular system, the gastrointestinal tract, and the immune system, detoxification of the body, and brain, liver and kidney functions. Hundreds of active substances, including valuable antioxidants, amino acids, vitamins and minerals, to make not only each day, but also each night special.\",\n",
      "  \"metadata\": {\n",
      "    \"domain\": \"Products\",\n",
      "    \"source_file\": \"Products.csv\",\n",
      "    \"primary_entity_id\": \"D1\",\n",
      "    \"primary_entity_name\": \"DuoLife Day and Night\",\n",
      "    \"type\": \"Set\",\n",
      "    \"link\": \"https://myduolife.com/shop/products/1/76,zestaw-duolife-dzien-i-noc.html?__language=pl\",\n",
      "    \"section\": \"Description\"\n",
      "  }\n",
      "}\n",
      "\n",
      "--- Example from Domain: Ingredients ---\n",
      "{\n",
      "  \"chunk_id\": \"9a8e7838-773d-472e-9bd3-1229a31761aa\",\n",
      "  \"text_to_embed\": \"Contains vitamins A, C, B1, B2, B6, PP, E, K, and folic acid, minerals Ca, Fe, Mg, P, K, Na, Zn, Mn, Se, F, simple sugars, and polyphenols like resveratrol and anthocyanins. Resveratrol has anti-inflammatory, antiangiogenic, antioxidative, and anticancer effects, protecting against cardiovascular and neurodegenerative diseases, inhibiting LDL cholesterol oxidation and platelet aggregation, and lowering blood pressure and viscosity. Anthocyanins have a cardioprotective effect.\",\n",
      "  \"metadata\": {\n",
      "    \"domain\": \"Ingredients\",\n",
      "    \"source_file\": \"Ingredients.csv\",\n",
      "    \"primary_entity_id\": \"I1\",\n",
      "    \"primary_entity_name\": \"Grape (Vitis vinifera) fruit extract\",\n",
      "    \"section\": \"Description\"\n",
      "  }\n",
      "}\n",
      "\n",
      "--- Example from Domain: Cosmetics ---\n",
      "{\n",
      "  \"chunk_id\": \"895f0808-5fc5-402a-a6b0-8f48a680d691\",\n",
      "  \"text_to_embed\": \"Actively moisturises, soothes, smooths and regenerates the skin of the hands. Regular use of Vita C hand cream helps to reduce the visibility of the first signs of skin aging.\",\n",
      "  \"metadata\": {\n",
      "    \"domain\": \"Cosmetics\",\n",
      "    \"source_file\": \"Cosmetics .csv\",\n",
      "    \"primary_entity_id\": \"1\",\n",
      "    \"primary_entity_name\": \"Pro Vita C Hand Cream\",\n",
      "    \"section\": \"Description\",\n",
      "    \"type\": \"Hand Cream\",\n",
      "    \"ingredients_list\": [\n",
      "      \"vitamin E\"\n",
      "    ]\n",
      "  }\n",
      "}\n",
      "\n",
      "--- Example from Domain: Business_Model ---\n",
      "{\n",
      "  \"chunk_id\": \"4db9a64d-1cdf-4b98-b8f1-860160d4b687\",\n",
      "  \"text_to_embed\": \"Question: Who is a Group Leader (hereafter referred to as GL) and how do you become one? Answer: The Group Leader is the first very important position in the Compensation Plan (Career Plan). All promotions begin from this point, which also leads to higher earnings. A Group Leader is a Club Member who has at least 250P of personal monthly activity and has a turnover of at least 500P in three separate lines (structures under them). (P stands for sales points \\u2013 in Poland, 1P equals approximately 2 PLN). The total turnover in a given line is considered, meaning that the 500P turnover can be achieved by a single person or a group of people (e.g., 2x250P). So, to reach the Group Leader level, you only need 3 Partners who have a monthly personal activity of 500P. Are these details sufficient for you? Is there anything else I can explain on this topic? If you'd like to read more, here is a link to the detailed Compensation Plan of DUOLIFE Club: https://myduolife.com/downloads/4062,file,plan-kompensacyjny-kompendium-wiedzy-power-point.pptx?language_id=pl\",\n",
      "  \"metadata\": {\n",
      "    \"domain\": \"Business_Model\",\n",
      "    \"source_file\": \"Old Business Q&A.csv\",\n",
      "    \"primary_entity_name\": \"Who is a Group Leader (hereafter referred to as GL) and how do you become one?\",\n",
      "    \"section\": \"Q&A\",\n",
      "    \"tags\": [\n",
      "      \"Group Leader\",\n",
      "      \"GL\",\n",
      "      \"Compensation Plan\",\n",
      "      \"Career Plan\",\n",
      "      \"promotion\",\n",
      "      \"earnings\",\n",
      "      \"Club Member\",\n",
      "      \"personal activity\",\n",
      "      \"turnover\",\n",
      "      \"sales points\",\n",
      "      \"structures\",\n",
      "      \"Partners\"\n",
      "    ]\n",
      "  }\n",
      "}\n",
      "\n",
      "Successfully saved all 757 structured data chunks to 'structured_rag_data.json'\n"
     ]
    }
   ],
   "source": [
    "# --- Final Summary ---\n",
    "print(f\"\\n--- PROCESSING COMPLETE ---\")\n",
    "print(f\"Total number of structured data chunks created: {len(all_chunks)}\")\n",
    "\n",
    "# --- Display a few examples to verify the output ---\n",
    "print(\"\\n--- EXAMPLE CHUNKS ---\")\n",
    "if len(all_chunks) > 0:\n",
    "    # Find and print one example from each domain to show the diversity of the data\n",
    "    domains_seen = set()\n",
    "    for chunk in all_chunks:\n",
    "        domain = chunk['metadata']['domain']\n",
    "        if domain not in domains_seen:\n",
    "            print(f\"\\n--- Example from Domain: {domain} ---\")\n",
    "            print(json.dumps(chunk, indent=2))\n",
    "            domains_seen.add(domain)\n",
    "        # Stop after showing one of each main domain type\n",
    "        if len(domains_seen) >= 4:\n",
    "            break\n",
    "else:\n",
    "    print(\"No chunks were created. Please check the input files and code.\")\n",
    "\n",
    "# --- Save the final data to a JSON file ---\n",
    "output_filename = 'structured_rag_data.json'\n",
    "try:\n",
    "    with open(output_filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(all_chunks, f, indent=4, ensure_ascii=False)\n",
    "    print(f\"\\nSuccessfully saved all {len(all_chunks)} structured data chunks to '{output_filename}'\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nAn error occurred while saving the file: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c0e873",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
